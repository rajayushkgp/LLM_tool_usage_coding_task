{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "sys.path.append('../src') \n",
    "\n",
    "from dataset_utils import Static_dataGen, Dynamic_dataGen, Bonus_dataGen, Preprocessing\n",
    "\n",
    "key = os.environ.get(\"OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Dataset Creation\n",
    "Aim: To generate a set of query-output pairs using the original set of 9 tools\n",
    "\n",
    "Method: \n",
    "1. A set of 3-4 tools is sampled every iteration for query generation\n",
    "2. The sampled set of tools is passed to an LLM agent for query generation\n",
    "3. The query is then passed to another agent, along with their descriptions, to generate its completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDatagen = Static_dataGen(key)\n",
    "\n",
    "no_of_StaticQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "data_dict = staticDatagen.genQuery(no_of_StaticQuery_CompletionPairs2beGen)\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "field_names= ['Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveStaticdataset.csv', 'w') as csv_file:  \n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=data_dict[0].keys())\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Dataset Creation\n",
    "Aim: To generate a dynamic toolset, and combining them with the original toolset to obtain a set of query-output pairs\n",
    "\n",
    "Method (Dynamic Toolset Creation): \n",
    "1. 4 tools are sampled from the original toolset every iteration\n",
    "2. These tools are then passed to an agent, to generate similar tools\n",
    "\n",
    "Method (Query-Output Pair Generation): \n",
    "1. Random 10 tools along with the original 9 at a time are passed to the agent for generating queries. The model has the liberty to select any number of tools from this for query generation. \n",
    "2. Another agent then generates the completions for the query list\n",
    "(The query list is cleaned by code and manual intervention before passing to the second agent, and a similar process is followed for the final CSV creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicDatagen = Dynamic_dataGen(key)\n",
    "\n",
    "no_of_newTool2beAdded = 10\n",
    "\n",
    "no_of_DynamicQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "dynamicDatagen.genDynamicTools(no_of_newTool2beAdded)\n",
    "\n",
    "data_dict = dynamicDatagen.genDynamicQueryOutputPair(no_of_DynamicQuery_CompletionPairs2beGen)\n",
    "\n",
    "field_names= ['Added_Tools','Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveDynamicData.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Dataset Creation\n",
    "Aim: To generate a set of query-output pairs which involves usage of conditional and iterative operators\n",
    "\n",
    "Method: Manually creating a list of 5 such query-output pairs, feeding these examples along with a list of a few relevant dynamic tools combined with the original toolset in the query-generating agent, and finally passing this list of queries in the completion agent. At every step of output from the model, the data is cleaned before saving and passing to the further agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusDatagen = Bonus_dataGen(key)\n",
    "\n",
    "no_of_BonusQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "data_dict = bonusDatagen.genBonusQueryOutputPair(no_of_BonusQuery_CompletionPairs2beGen)\n",
    "\n",
    "field_names= ['Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveBonusData.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructuring Dataset For Different Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset formation for P1 Pipeline\n",
    "\n",
    "Since the P1 pipeline does not require a training set, the following code generates an evaluation dataset for the P1 pipeline. The docstring is created by choosing the tools used in the query along with some random tools from the tools list. Since the data has to be used for infering the model, and has no prior knowledge of the tools, it requires the docstring of the allowed tools, along with some examples (few-shot) in the prompt to generate good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\") \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "#Static\n",
    "staticDictP1 = []\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P1_datasets'):\n",
    "    os.makedirs('../datasets/Generated/P1_datasets')\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    added_tools = datasetForm.p1_static()\n",
    "    staticDictP1.append({'Query': query,'Output' : output, 'Docstring': added_tools})\n",
    "\n",
    "field_names= ['Query', 'Output', 'Docstring']\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/StaticP1dataset.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(staticDictP1)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP1 = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    added_tools = datasetForm.p1_dynamic(additional_tools)\n",
    "    dynamicDictP1.append({'Query': query,'Output' : output, 'Docstring': added_tools})\n",
    "\n",
    "field_names= ['Query', 'Output', 'Docstring']\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/DynamicP1dataset.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamicDictP1)\n",
    "\n",
    "# Bonus\n",
    "bonusDictP1 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    added_tools = datasetForm.p1_bonus(bonusTool_list)\n",
    "    bonusDictP1.append({'Query': query,'Output' : output, 'Docstring': added_tools})\n",
    "\n",
    "field_names= ['Query', 'Output', 'Docstring']\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/BonusP1dataset.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonusDictP1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDictP2 = []\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output)\n",
    "    staticDictP2.append({'Prompt':prompt})\n",
    "\n",
    "field_names= ['Prompt']\n",
    "\n",
    "with open('./datasetForm/StaticP2prompt.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(staticDictP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDictP3 = []\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output)\n",
    "    staticDictP3.append({'Prompt':prompt})\n",
    "\n",
    "field_names = ['Prompt']\n",
    "\n",
    "with open('./datasetForm/StaticP3prompt.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(staticDictP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Dataset Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicDictP2 = []\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output,additional_tools)\n",
    "    dynamicDictP2.append({'Prompt':prompt})\n",
    "\n",
    "field_names= ['Prompt']\n",
    "\n",
    "with open('./datasetForm/DynamicP2prompt.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamicDictP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicDictP3 = []\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output,additional_tools)\n",
    "    dynamicDictP3.append({'Prompt':prompt})\n",
    "\n",
    "field_names= ['Prompt']\n",
    "\n",
    "with open('./datasetForm/DynamicP3prompt.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamicDictP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Dataset Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create tasks from the text \"WeeklyUpdate\" and ...</td>\n",
       "      <td>var_1 = who_am_i()\\nvar_2 = create_actionable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retrieve work items with type \"task\" and sever...</td>\n",
       "      <td>for loop_var in range(0,10):\\n    temp_1 = wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find work items with priority \"p1\" and type \"i...</td>\n",
       "      <td>var_1 = works_list(issue.priority=[\"p1\"], type...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extract tasks from the text \"ReleaseNotes\", pr...</td>\n",
       "      <td>var_1 = create_actionable_tasks_from_text(text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fetch tasks for user \"USER-999\", prioritize th...</td>\n",
       "      <td>for loop_var in range(0,2):\\n    temp_1 = fetc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  Create tasks from the text \"WeeklyUpdate\" and ...   \n",
       "1  Retrieve work items with type \"task\" and sever...   \n",
       "2  Find work items with priority \"p1\" and type \"i...   \n",
       "3  Extract tasks from the text \"ReleaseNotes\", pr...   \n",
       "4  Fetch tasks for user \"USER-999\", prioritize th...   \n",
       "\n",
       "                                              Output  \n",
       "0  var_1 = who_am_i()\\nvar_2 = create_actionable_...  \n",
       "1  for loop_var in range(0,10):\\n    temp_1 = wor...  \n",
       "2  var_1 = works_list(issue.priority=[\"p1\"], type...  \n",
       "3  var_1 = create_actionable_tasks_from_text(text...  \n",
       "4  for loop_var in range(0,2):\\n    temp_1 = fetc...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset formation for P1 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusDictP2 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output,bonusTool_list)\n",
    "    bonusDictP2.append({'Prompt':prompt})\n",
    "\n",
    "field_names= ['Prompt']\n",
    "\n",
    "with open('./datasetForm/BonusP2prompt.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonusDictP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusDictP3 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output,bonusTool_list)\n",
    "    bonusDictP3.append({'Prompt':prompt})\n",
    "\n",
    "field_names= ['Prompt']\n",
    "\n",
    "with open('./datasetForm/BonusP3prompt.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonusDictP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Validation Test For Static Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasetForm/StaticP2prompt.csv\")\n",
    "\n",
    "train_df = df[0:1700]\n",
    "validation_df = df[1700:1900]\n",
    "test_df = df[1900:]\n",
    "\n",
    "train_df.to_csv(\"./finetuning_P2dataset/train.csv\", index=False)\n",
    "validation_df.to_csv(\"./finetuning_P2dataset/validation.csv\", index=False)\n",
    "test_df.to_csv(\"./finetuning_P2dataset/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasetForm/StaticP3prompt.csv\")\n",
    "\n",
    "train_df = df[0:1700]\n",
    "validation_df = df[1700:1900]\n",
    "test_df = df[1900:]\n",
    "\n",
    "train_df.to_csv(\"./finetuning_P3dataset/train.csv\", index=False)\n",
    "validation_df.to_csv(\"./finetuning_P3dataset/validation.csv\", index=False)\n",
    "test_df.to_csv(\"./finetuning_P3dataset/test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
